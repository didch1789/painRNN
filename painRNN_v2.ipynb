{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1 was just vanila.\n",
    "\n",
    "# v2: 24.09.01. \n",
    "#   1. Doing the CV (10 folds)\n",
    "#   2. Early stopping with patience parameter.\n",
    "#   3. Takings n steps into account. \n",
    "\n",
    "# v3: not yet\n",
    "#   1. Doing the CV (10 folds)\n",
    "#   2. Early stopping with patience parameter.\n",
    "#   3. Takings n steps into account. not by h(t-1) + h(t-2), but with different weights.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "take_n_steps = 5\n",
    "\n",
    "# Function to set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "seed = 240802\n",
    "set_seed(seed)\n",
    "\n",
    "# Define the RNN model that takes two previous hidden states\n",
    "# Define the custom RNN model that takes more previous hidden states into account\n",
    "class PainRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_previous_steps=take_n_steps):\n",
    "        super(PainRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_previous_steps = num_previous_steps  # Number of previous steps to consider\n",
    "\n",
    "        # Define layers\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # Initialize outputs and hidden states list\n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        \n",
    "        if self.num_previous_steps != 0:\n",
    "            # Initialize hidden states for the specified number of previous steps\n",
    "            past_hidden_states = [torch.zeros(batch_size, self.hidden_size, device=x.device) \n",
    "                                for _ in range(self.num_previous_steps)]\n",
    "\n",
    "            # Iterate through the sequence\n",
    "            for t in range(seq_len):\n",
    "                # Current input\n",
    "                x_t = x[:, t, :]\n",
    "\n",
    "                # Sum all past hidden states to compute the current hidden state\n",
    "                combined_hidden = sum(past_hidden_states)\n",
    "\n",
    "                # Update current hidden state\n",
    "                h_t = self.rnn_cell(x_t, combined_hidden)\n",
    "\n",
    "                # Update the list of past hidden states\n",
    "                past_hidden_states.pop(0)  # Remove the oldest hidden state\n",
    "                past_hidden_states.append(h_t)  # Add the current hidden state\n",
    "\n",
    "                # Save the current hidden state\n",
    "                hidden_states.append(h_t)\n",
    "\n",
    "                # Compute output for the current time step\n",
    "                output = self.fc(h_t)\n",
    "                outputs.append(output)\n",
    "\n",
    "            # Stack outputs and hidden states to match batch_first=True format\n",
    "            outputs = torch.stack(outputs, dim=1)\n",
    "            hidden_states = torch.stack(hidden_states, dim=1)\n",
    "            \n",
    "        else:\n",
    "            for t in range(seq_len):\n",
    "                # Current input\n",
    "                x_t = x[:, t, :]\n",
    "                if t == 0:\n",
    "                    h_t_1 = torch.zeros(batch_size, self.hidden_size, device=x.device)\n",
    "                else:\n",
    "                    h_t_1 = h_t\n",
    "                h_t = self.rnn_cell(x_t, h_t_1)\n",
    "                hidden_states.append(h_t)  # Add the current hidden state\n",
    "                output = self.fc(h_t)\n",
    "                outputs.append(output)\n",
    "            outputs = torch.stack(outputs, dim=1)\n",
    "            hidden_states = torch.stack(hidden_states, dim=1)\n",
    "            \n",
    "        return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Avg Train Loss: 112.4835, Avg Val Loss: 110.4013\n",
      "Epoch [2/500], Avg Train Loss: 85.7206, Avg Val Loss: 84.2121\n",
      "Epoch [3/500], Avg Train Loss: 68.7737, Avg Val Loss: 67.8314\n",
      "Epoch [4/500], Avg Train Loss: 55.4749, Avg Val Loss: 54.9152\n",
      "Epoch [5/500], Avg Train Loss: 45.7992, Avg Val Loss: 45.2683\n",
      "Epoch [6/500], Avg Train Loss: 38.9361, Avg Val Loss: 38.6707\n",
      "Epoch [7/500], Avg Train Loss: 33.8515, Avg Val Loss: 33.7241\n",
      "Epoch [8/500], Avg Train Loss: 30.0004, Avg Val Loss: 29.8403\n",
      "Epoch [9/500], Avg Train Loss: 27.1780, Avg Val Loss: 27.1232\n",
      "Epoch [10/500], Avg Train Loss: 25.3284, Avg Val Loss: 25.1390\n",
      "Epoch [11/500], Avg Train Loss: 24.0373, Avg Val Loss: 23.9479\n",
      "Epoch [12/500], Avg Train Loss: 23.1262, Avg Val Loss: 23.0725\n",
      "Epoch [13/500], Avg Train Loss: 22.5230, Avg Val Loss: 22.5797\n",
      "Epoch [14/500], Avg Train Loss: 22.0938, Avg Val Loss: 22.1978\n",
      "Epoch [15/500], Avg Train Loss: 21.8780, Avg Val Loss: 22.1182\n",
      "Epoch [16/500], Avg Train Loss: 21.6867, Avg Val Loss: 21.7296\n",
      "Epoch [17/500], Avg Train Loss: 21.5692, Avg Val Loss: 21.6036\n",
      "Epoch [18/500], Avg Train Loss: 21.5051, Avg Val Loss: 21.5247\n",
      "Epoch [19/500], Avg Train Loss: 21.3353, Avg Val Loss: 21.4311\n",
      "Epoch [20/500], Avg Train Loss: 21.3257, Avg Val Loss: 21.3848\n",
      "Epoch [21/500], Avg Train Loss: 21.2152, Avg Val Loss: 21.3012\n",
      "Epoch [22/500], Avg Train Loss: 21.2277, Avg Val Loss: 21.1280\n",
      "Epoch [23/500], Avg Train Loss: 21.2831, Avg Val Loss: 21.1825\n",
      "Epoch [24/500], Avg Train Loss: 21.2737, Avg Val Loss: 21.0478\n",
      "Epoch [25/500], Avg Train Loss: 21.2383, Avg Val Loss: 21.2807\n",
      "Epoch [26/500], Avg Train Loss: 21.1714, Avg Val Loss: 21.1351\n",
      "Epoch [27/500], Avg Train Loss: 21.2026, Avg Val Loss: 21.2312\n",
      "Epoch [28/500], Avg Train Loss: 21.1870, Avg Val Loss: 21.1130\n",
      "Epoch [29/500], Avg Train Loss: 21.2333, Avg Val Loss: 21.2495\n",
      "Epoch [30/500], Avg Train Loss: 21.1644, Avg Val Loss: 21.0758\n",
      "Epoch [31/500], Avg Train Loss: 21.1614, Avg Val Loss: 21.1086\n",
      "Epoch [32/500], Avg Train Loss: 21.1058, Avg Val Loss: 21.1168\n",
      "Epoch [33/500], Avg Train Loss: 21.1639, Avg Val Loss: 21.1070\n",
      "Epoch [34/500], Avg Train Loss: 21.1329, Avg Val Loss: 21.0443\n",
      "Epoch [35/500], Avg Train Loss: 21.1122, Avg Val Loss: 21.1433\n",
      "Epoch [36/500], Avg Train Loss: 21.1645, Avg Val Loss: 21.1441\n",
      "Epoch [37/500], Avg Train Loss: 21.1047, Avg Val Loss: 21.0932\n",
      "Epoch [38/500], Avg Train Loss: 21.1965, Avg Val Loss: 21.0574\n",
      "Epoch [39/500], Avg Train Loss: 21.1501, Avg Val Loss: 21.1521\n",
      "Epoch [40/500], Avg Train Loss: 21.1026, Avg Val Loss: 21.1278\n",
      "Epoch [41/500], Avg Train Loss: 21.0943, Avg Val Loss: 21.0103\n",
      "Epoch [42/500], Avg Train Loss: 21.1218, Avg Val Loss: 21.2276\n",
      "Epoch [43/500], Avg Train Loss: 21.1394, Avg Val Loss: 21.0337\n",
      "Epoch [44/500], Avg Train Loss: 21.0939, Avg Val Loss: 21.2309\n",
      "Epoch [45/500], Avg Train Loss: 21.0961, Avg Val Loss: 21.2081\n",
      "Epoch [46/500], Avg Train Loss: 21.0803, Avg Val Loss: 21.1133\n",
      "Epoch [47/500], Avg Train Loss: 21.0953, Avg Val Loss: 21.1209\n",
      "Epoch [48/500], Avg Train Loss: 21.0778, Avg Val Loss: 20.9895\n",
      "Epoch [49/500], Avg Train Loss: 21.1448, Avg Val Loss: 21.1884\n",
      "Epoch [50/500], Avg Train Loss: 21.0963, Avg Val Loss: 21.1403\n",
      "Epoch [51/500], Avg Train Loss: 21.2048, Avg Val Loss: 21.1894\n",
      "Epoch [52/500], Avg Train Loss: 21.1343, Avg Val Loss: 21.1852\n",
      "Epoch [53/500], Avg Train Loss: 21.1385, Avg Val Loss: 21.0662\n",
      "Epoch [54/500], Avg Train Loss: 21.1083, Avg Val Loss: 21.0683\n",
      "Epoch [55/500], Avg Train Loss: 21.1357, Avg Val Loss: 21.0628\n",
      "Epoch [56/500], Avg Train Loss: 21.1143, Avg Val Loss: 21.1820\n",
      "Epoch [57/500], Avg Train Loss: 21.0750, Avg Val Loss: 21.1370\n",
      "Epoch [58/500], Avg Train Loss: 21.1329, Avg Val Loss: 21.0724\n",
      "Early stopping triggered after 58 epochs.\n",
      "Training & Saving complete!\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "input_size = 1  # Input feature size\n",
    "hidden_size = 8  # Number of features in the hidden state\n",
    "output_size = 3  # Output feature size\n",
    "learning_rate = 1e-3  # [0.001 or 0.0001]\n",
    "num_epochs = 500\n",
    "k_folds = 10\n",
    "n_batch = 32\n",
    "patience = 10  # Number of epochs to wait for improvement before stopping\n",
    "\n",
    "# Load input and output data\n",
    "inputoutputs = sio.loadmat('InOutputs')\n",
    "Inputs = inputoutputs['Inputs']\n",
    "Outputs = inputoutputs['Outputs']\n",
    "\n",
    "Xnp = []\n",
    "ynp = []\n",
    "for cond_i in range(len(Outputs)):\n",
    "    Xnp.append(np.tile(Inputs[cond_i, :], (Outputs[cond_i][0].shape[0], 1)))\n",
    "    ynp.append(Outputs[cond_i][0])\n",
    "Xnp = np.expand_dims(np.vstack(Xnp), axis=2)\n",
    "ynp = np.vstack(ynp)\n",
    "X = torch.from_numpy(Xnp).to(torch.float32)\n",
    "y = torch.from_numpy(ynp).to(torch.float32)\n",
    "\n",
    "# Combine inputs and outputs into a dataset\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)\n",
    "\n",
    "# Lists to store training losses, validation losses, hidden states, and outputs\n",
    "train_losses_all_epochs = []\n",
    "val_losses_all_epochs = []\n",
    "fold_hidden_states = []  # Store hidden states for each fold\n",
    "fold_outputs = []  # Store outputs for each fold\n",
    "\n",
    "# Model instantiation\n",
    "device = 'cpu'\n",
    "model = PainRNN(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "best_model_state = None\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Reset for each epoch\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "    hidden_states_all_epochs = []  # To store hidden states of all epochs\n",
    "    outputs_all_epochs = []  # To store outputs of all epochs\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        \n",
    "        # Sample elements randomly from a given list of indices, no replacement.\n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_idx)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_idx)\n",
    "\n",
    "        # Define data loaders for training and validation\n",
    "        train_loader = DataLoader(dataset, batch_size=n_batch, sampler=train_subsampler)\n",
    "        val_loader = DataLoader(dataset, batch_size=n_batch, sampler=val_subsampler)\n",
    "\n",
    "        # Training loop for current fold\n",
    "        model.train()  # Set the model to training mode\n",
    "        fold_train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs, hidden_states = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            fold_train_loss += loss.item()\n",
    "\n",
    "            # Save hidden states and outputs\n",
    "            hidden_states_all_epochs.append(hidden_states.detach().cpu().numpy())\n",
    "            outputs_all_epochs.append(outputs.detach().cpu().numpy())\n",
    "\n",
    "        fold_train_loss /= len(train_loader)\n",
    "        epoch_train_loss += fold_train_loss\n",
    "\n",
    "        # Validation loop for current fold\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        fold_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs, _ = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                fold_val_loss += loss.item()\n",
    "\n",
    "        fold_val_loss /= len(val_loader)\n",
    "        epoch_val_loss += fold_val_loss\n",
    "\n",
    "    # Average train and validation loss across all folds\n",
    "    epoch_train_loss /= k_folds\n",
    "    epoch_val_loss /= k_folds\n",
    "\n",
    "    train_losses_all_epochs.append(epoch_train_loss)\n",
    "    val_losses_all_epochs.append(epoch_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Avg Train Loss: {epoch_train_loss:.4f}, Avg Val Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "    # Early stopping check\n",
    "    if epoch_val_loss < best_val_loss:\n",
    "        best_val_loss = epoch_val_loss\n",
    "        best_model_state = model.state_dict()  # Save the best model state\n",
    "        epochs_without_improvement = 0\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= patience:\n",
    "        print(f'Early stopping triggered after {epoch + 1} epochs.')\n",
    "        break\n",
    "\n",
    "# Load the best model state (optional, if you want to use the best model after early stopping)\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "out_t, h_t = model(torch.from_numpy(np.expand_dims(Inputs, axis = 2)).to(torch.float32))\n",
    "out_t = out_t.detach().numpy()\n",
    "h_t   = h_t.detach().numpy()\n",
    "\n",
    "# Save the hidden states, outputs, and losses using scipy.io.savemat\n",
    "sio.savemat('HiddenLayers.mat', {\n",
    "    'out_t': out_t, 'h_t': h_t, \n",
    "    'val_losses': val_losses_all_epochs,\n",
    "    'hidden_states': hidden_states_all_epochs,\n",
    "    'inputs': Inputs,\n",
    "})\n",
    "print(\"Training & Saving complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.4 ('env_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "76704af79bdacbe9e3295deab26431e84c2830fb3934619e845f35b49385e555"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
