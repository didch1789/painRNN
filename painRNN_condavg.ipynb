{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.8419 in 1 of 10\n",
      "Epoch [20/100], Loss: 0.8076 in 1 of 10\n",
      "Epoch [30/100], Loss: 0.7778 in 1 of 10\n",
      "Epoch [40/100], Loss: 0.7527 in 1 of 10\n",
      "Epoch [50/100], Loss: 0.7319 in 1 of 10\n",
      "Epoch [60/100], Loss: 0.7150 in 1 of 10\n",
      "Epoch [70/100], Loss: 0.7016 in 1 of 10\n",
      "Epoch [80/100], Loss: 0.6910 in 1 of 10\n",
      "Epoch [90/100], Loss: 0.6828 in 1 of 10\n",
      "Epoch [100/100], Loss: 0.6765 in 1 of 10\n",
      "Epoch [10/100], Loss: 0.7082 in 2 of 10\n",
      "Epoch [20/100], Loss: 0.7142 in 2 of 10\n",
      "Epoch [30/100], Loss: 0.6881 in 2 of 10\n",
      "Epoch [40/100], Loss: 0.6629 in 2 of 10\n",
      "Epoch [50/100], Loss: 0.6488 in 2 of 10\n",
      "Epoch [60/100], Loss: 0.6319 in 2 of 10\n",
      "Epoch [70/100], Loss: 0.6198 in 2 of 10\n",
      "Epoch [80/100], Loss: 0.6083 in 2 of 10\n",
      "Epoch [90/100], Loss: 0.5990 in 2 of 10\n",
      "Epoch [100/100], Loss: 0.5864 in 2 of 10\n",
      "Epoch [10/100], Loss: 0.8018 in 3 of 10\n",
      "Epoch [20/100], Loss: 0.7262 in 3 of 10\n",
      "Epoch [30/100], Loss: 0.7129 in 3 of 10\n",
      "Epoch [40/100], Loss: 0.6894 in 3 of 10\n",
      "Epoch [50/100], Loss: 0.6643 in 3 of 10\n",
      "Epoch [60/100], Loss: 0.6428 in 3 of 10\n",
      "Epoch [70/100], Loss: 0.6245 in 3 of 10\n",
      "Epoch [80/100], Loss: 0.6094 in 3 of 10\n",
      "Epoch [90/100], Loss: 0.5999 in 3 of 10\n",
      "Epoch [100/100], Loss: 0.5918 in 3 of 10\n",
      "Epoch [10/100], Loss: 0.9777 in 4 of 10\n",
      "Epoch [20/100], Loss: 0.7904 in 4 of 10\n",
      "Epoch [30/100], Loss: 0.6983 in 4 of 10\n",
      "Epoch [40/100], Loss: 0.6564 in 4 of 10\n",
      "Epoch [50/100], Loss: 0.6174 in 4 of 10\n",
      "Epoch [60/100], Loss: 0.5987 in 4 of 10\n",
      "Epoch [70/100], Loss: 0.5863 in 4 of 10\n",
      "Epoch [80/100], Loss: 0.5737 in 4 of 10\n",
      "Epoch [90/100], Loss: 0.5590 in 4 of 10\n",
      "Epoch [100/100], Loss: 0.5515 in 4 of 10\n",
      "Epoch [10/100], Loss: 0.8453 in 5 of 10\n",
      "Epoch [20/100], Loss: 0.8251 in 5 of 10\n",
      "Epoch [30/100], Loss: 0.7983 in 5 of 10\n",
      "Epoch [40/100], Loss: 0.7078 in 5 of 10\n",
      "Epoch [50/100], Loss: 0.6554 in 5 of 10\n",
      "Epoch [60/100], Loss: 0.6292 in 5 of 10\n",
      "Epoch [70/100], Loss: 0.6185 in 5 of 10\n",
      "Epoch [80/100], Loss: 0.6119 in 5 of 10\n",
      "Epoch [90/100], Loss: 0.6071 in 5 of 10\n",
      "Epoch [100/100], Loss: 0.6046 in 5 of 10\n",
      "Epoch [10/100], Loss: 1.2004 in 6 of 10\n",
      "Epoch [20/100], Loss: 1.0668 in 6 of 10\n",
      "Epoch [30/100], Loss: 0.7036 in 6 of 10\n",
      "Epoch [40/100], Loss: 0.7086 in 6 of 10\n",
      "Epoch [50/100], Loss: 0.6639 in 6 of 10\n",
      "Epoch [60/100], Loss: 0.6041 in 6 of 10\n",
      "Epoch [70/100], Loss: 0.5908 in 6 of 10\n",
      "Epoch [80/100], Loss: 0.5792 in 6 of 10\n",
      "Epoch [90/100], Loss: 0.5714 in 6 of 10\n",
      "Epoch [100/100], Loss: 0.5648 in 6 of 10\n",
      "Epoch [10/100], Loss: 0.9229 in 7 of 10\n",
      "Epoch [20/100], Loss: 0.7315 in 7 of 10\n",
      "Epoch [30/100], Loss: 0.7299 in 7 of 10\n",
      "Epoch [40/100], Loss: 0.6886 in 7 of 10\n",
      "Epoch [50/100], Loss: 0.6826 in 7 of 10\n",
      "Epoch [60/100], Loss: 0.6640 in 7 of 10\n",
      "Epoch [70/100], Loss: 0.6556 in 7 of 10\n",
      "Epoch [80/100], Loss: 0.6490 in 7 of 10\n",
      "Epoch [90/100], Loss: 0.6436 in 7 of 10\n",
      "Epoch [100/100], Loss: 0.6352 in 7 of 10\n",
      "Epoch [10/100], Loss: 1.2491 in 8 of 10\n",
      "Epoch [20/100], Loss: 0.7802 in 8 of 10\n",
      "Epoch [30/100], Loss: 0.6825 in 8 of 10\n",
      "Epoch [40/100], Loss: 0.6296 in 8 of 10\n",
      "Epoch [50/100], Loss: 0.6045 in 8 of 10\n",
      "Epoch [60/100], Loss: 0.6015 in 8 of 10\n",
      "Epoch [70/100], Loss: 0.5802 in 8 of 10\n",
      "Epoch [80/100], Loss: 0.5672 in 8 of 10\n",
      "Epoch [90/100], Loss: 0.5466 in 8 of 10\n",
      "Epoch [100/100], Loss: 0.5380 in 8 of 10\n",
      "Epoch [10/100], Loss: 1.2300 in 9 of 10\n",
      "Epoch [20/100], Loss: 0.8226 in 9 of 10\n",
      "Epoch [30/100], Loss: 0.7211 in 9 of 10\n",
      "Epoch [40/100], Loss: 0.6466 in 9 of 10\n",
      "Epoch [50/100], Loss: 0.6274 in 9 of 10\n",
      "Epoch [60/100], Loss: 0.6140 in 9 of 10\n",
      "Epoch [70/100], Loss: 0.6040 in 9 of 10\n",
      "Epoch [80/100], Loss: 0.5953 in 9 of 10\n",
      "Epoch [90/100], Loss: 0.5885 in 9 of 10\n",
      "Epoch [100/100], Loss: 0.5816 in 9 of 10\n",
      "Epoch [10/100], Loss: 2.2770 in 10 of 10\n",
      "Epoch [20/100], Loss: 1.2644 in 10 of 10\n",
      "Epoch [30/100], Loss: 0.7799 in 10 of 10\n",
      "Epoch [40/100], Loss: 0.7649 in 10 of 10\n",
      "Epoch [50/100], Loss: 0.6983 in 10 of 10\n",
      "Epoch [60/100], Loss: 0.6658 in 10 of 10\n",
      "Epoch [70/100], Loss: 0.6504 in 10 of 10\n",
      "Epoch [80/100], Loss: 0.6390 in 10 of 10\n",
      "Epoch [90/100], Loss: 0.6332 in 10 of 10\n",
      "Epoch [100/100], Loss: 0.6249 in 10 of 10\n"
     ]
    }
   ],
   "source": [
    "# v1 was just vanila.\n",
    "\n",
    "# v2: 24.09.01. \n",
    "#   1. Doing the CV (10 folds)\n",
    "#   2. Early stopping with patience parameter.\n",
    "#   3. Takings n steps into account. \n",
    "\n",
    "# v3: 24.10.15 - 29\n",
    "#   1. No CV. No Early Stopping as well. \n",
    "#       Adding early stopping may be an option, but it it not super necessary.\n",
    "#   2. Adding tau param. h(t) = f((1-tau) * h(t-1) + tau * x(t)) => This is what basically CT-RNN is about.\n",
    "\n",
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seed random number generator for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "## ---------------------------------- ##\n",
    "## ------------ FIX down ------------ ##\n",
    "## ---------------------------------- ##\n",
    "\n",
    "# input\n",
    "input_size  = 1   # Input feature size\n",
    "hidden_size = 16  # Number of features in the hidden state\n",
    "output_size = 2   # Output feature size (stim rating / click profiles)\n",
    "output_name = ['stim_rating', 'clicks'] # 'stim_rating' 'clicks'\n",
    "taus        = np.linspace(0, 1, 10)\n",
    "noise_sig   = 0.5\n",
    "\n",
    "# params\n",
    "learning_rate = 1e-2 # \n",
    "n_epochs      = 100\n",
    "batch_size    = 9   # or 9 for condition. so far at the trial-level\n",
    "device        = 'cpu' # mps\n",
    "targfolder    = 'condavg_label-RatingClicks_loss-MSEBCE'\n",
    "if not (os.path.isdir(targfolder)):\n",
    "    os.mkdir(targfolder)\n",
    "\n",
    "## ---------------------------------- ##\n",
    "## ------------- FIX up ------------- ##\n",
    "## ---------------------------------- ##\n",
    "\n",
    "# Q. why no Cross-Validation?\n",
    "# A. no differences across inputs.\n",
    "\n",
    "# Input: n_batch, n_seqlen, n_dim\n",
    "class PainRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, tau):\n",
    "        super().__init__()\n",
    "        self.input_size  = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.alpha       = tau # [0, 1] balances the effect of the input and hidden.\n",
    "                               # having it as one is just same with vanila RNN\n",
    "        self.i2h         = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h         = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o         = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, init_hidden=None):\n",
    "        hidden_units  = []\n",
    "        output_units  = []\n",
    "        seq_len       = input.size(1) # sequence length\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "             if (i == 0) and (init_hidden is None):\n",
    "                 h_t = torch.zeros(input.size(0), self.hidden_size)\n",
    "             elif (i == 0) and (init_hidden is not None):\n",
    "                 h_t = init_hidden\n",
    "                 \n",
    "             h_t           = h_t.to(device)    \n",
    "             x_t           = input[:, i, :]\n",
    "             hidden_unit   = torch.relu(self.i2h(x_t) + self.h2h(h_t))\n",
    "             hidden_unit   = h_t * (1 - self.alpha) + hidden_unit * self.alpha\n",
    "             output_unit   = torch.sigmoid(self.h2o(hidden_unit))\n",
    "            \n",
    "             hidden_units.append(hidden_unit)\n",
    "             output_units.append(output_unit)\n",
    "        \n",
    "        hidden_units = torch.stack(hidden_units, dim = 1)\n",
    "        output_units = torch.stack(output_units, dim = 1)\n",
    "        return output_units, hidden_units\n",
    "# Load input and output data\n",
    "inputoutputs = sio.loadmat('InOutputs_condavg')\n",
    "Inputs       = inputoutputs['Inputs']\n",
    "Outputs      = inputoutputs['Outputs']\n",
    "\n",
    "Xnp = []\n",
    "ynp = []\n",
    "for cond_i in range(len(Outputs)):\n",
    "    ynp.append(Outputs[cond_i][0])\n",
    "\n",
    "Xnp = np.expand_dims(Inputs, axis = 2)\n",
    "ynp = np.stack(ynp, axis = 0)\n",
    "    \n",
    "X = torch.from_numpy(Xnp).to(torch.float32)\n",
    "y = torch.from_numpy(ynp).to(torch.float32)\n",
    "\n",
    "# Combine inputs and outputs into a dataset\n",
    "dataset    = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model instantiation\n",
    "for i, tau in enumerate(taus):\n",
    "    addstr      = 'tau_%.2f' % tau\n",
    "    pain_rnn    = PainRNN(input_size, hidden_size, output_size, tau).to(device)\n",
    "    optimizer   = optim.Adam(pain_rnn.parameters(), lr=learning_rate)\n",
    "    criterion1  = nn.MSELoss()              # For intensity.\n",
    "    criterion2  = nn.BCELoss()     # For clicks.\n",
    "    loss_epochs = []\n",
    "    for epoch in range(n_epochs):\n",
    "        for inputs, targets in dataloader:\n",
    "            add_noise = np.random.normal(0, noise_sig, inputs.shape)\n",
    "            inputs += torch.from_numpy(add_noise).to(torch.float32)\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs, _ = pain_rnn(inputs)  # Capture both outputs and hidden states\n",
    "            loss1 = criterion1(outputs[:, :, 0], targets[:, :, 0])\n",
    "            loss2 = criterion2(outputs[:, :, 1], targets[:, :, 1])\n",
    "            loss  = loss1 + loss2\n",
    "            # loss = criterion1(outputs, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        loss_epochs.append(loss.item())\n",
    "        \n",
    "        if ((epoch+1) % 10) == 0:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f} in {i+1} of {len(taus)}')\n",
    "            \n",
    "    out_t, h_t = pain_rnn(torch.from_numpy(np.expand_dims(Inputs, axis = 2)).to(torch.float32))\n",
    "    out_t      = out_t.detach().numpy()\n",
    "    h_t        = h_t.detach().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize = (9, 9))\n",
    "    axes[0, 0].plot(loss_epochs)\n",
    "    axes[0, 0].set_xlabel('Epochs')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "\n",
    "    axes[0, 1].plot(np.mean(h_t, 0))\n",
    "    axes[0, 1].set_xlabel('time')\n",
    "    axes[0, 1].set_ylabel('Hidden Unit activations')\n",
    "    \n",
    "    axes[2, 0].plot(Inputs.T)\n",
    "    axes[2, 0].set_xlabel('time')\n",
    "    axes[2, 0].set_ylabel('Training Input')\n",
    "    \n",
    "    for i in range(output_size):\n",
    "        axes[1, i].plot(out_t[:, :, i].T)\n",
    "        axes[1, i].set_xlabel('time')\n",
    "        axes[1, i].set_ylabel(f'Outputs {output_name[i]}')\n",
    "        \n",
    "        axes[2, i+1].imshow(y[:, :, i])\n",
    "        axes[2, i+1].set_xlabel('time')\n",
    "        axes[2, i+1].set_ylabel(f'Training {output_name[i]}')\n",
    "        \n",
    "    \n",
    "    sio.savemat(os.path.join(targfolder, ('HiddenLayers_' + addstr + '.mat')), \n",
    "                {'out_t': out_t, 'h_t': h_t, 'loss_epochs': loss_epochs, \n",
    "                 'i2h':pain_rnn.i2h.weight.detach().numpy(), \n",
    "                 'h2h':pain_rnn.h2h.weight.detach().numpy(), \n",
    "                 'h2o':pain_rnn.h2o.weight.detach().numpy()})\n",
    "\n",
    "    plt.savefig(os.path.join(targfolder, ('HiddenLayers_' + addstr + '.png')))\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
