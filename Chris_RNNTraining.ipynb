{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script trains an E/I continious time recurrent neural network to perform a change detection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seed random number generator for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# number of networks \n",
    "num_nets = 50; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up function to generate supervised dataset for change detection task and EI continious time RNN modified from code supplied in Yang & Wang (2020) https://www.cell.com/neuron/fulltext/S0896-6273(20)30705-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change detection environment\n",
    "def ChangeEnvGen(direction):\n",
    "\n",
    "    trial_length = 50\n",
    "\n",
    "    # inputs\n",
    "    inputs = np.zeros([2,trial_length])\n",
    "\n",
    "    if direction == 0:\n",
    "        inputs[0,:] += np.linspace(1,0,trial_length).T\n",
    "        inputs[1,:] += np.linspace(0,1,trial_length).T\n",
    "    else:\n",
    "        inputs[0,:] += np.linspace(0,1,trial_length).T\n",
    "        inputs[1,:] += np.linspace(1,0,trial_length).T\n",
    "        \n",
    "    # trial info\n",
    "    labels = np.zeros([1,trial_length])\n",
    "    if direction == 0:\n",
    "        labels[:,:25] = 0; labels[:,25:] = 1\n",
    "    else:\n",
    "        labels[:,:25] = 1; labels[:,25:] = 0\n",
    "\n",
    "    return inputs, labels, direction\n",
    "\n",
    "# EI CTRNN\n",
    "\n",
    "class PosWLinear(nn.Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    Same as nn.Linear, except that weight matrix is constrained to be non-negative\n",
    "    \"\"\"\n",
    "    __constants__ = ['bias', 'in_features', 'out_features']\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=False):\n",
    "        super(PosWLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # weight is non-negative\n",
    "        return F.linear(input, torch.abs(self.weight), self.bias)\n",
    "    \n",
    "    \n",
    "class EIRecLinear(nn.Module):\n",
    "    r\"\"\"Recurrent E-I Linear transformation.\n",
    "    \n",
    "    Args:\n",
    "        hidden_size: int, layer size\n",
    "        e_prop: float between 0 and 1, proportion of excitatory units\n",
    "    \"\"\"\n",
    "    __constants__ = ['bias', 'hidden_size', 'e_prop']\n",
    "\n",
    "    def __init__(self,hidden_size, e_prop, diag, bias=False):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.e_prop = e_prop\n",
    "        self.e_size = int(e_prop * hidden_size)\n",
    "        self.i_size = hidden_size - self.e_size\n",
    "        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))\n",
    "        mask = np.tile([1]*self.e_size+[-1]*self.i_size, (hidden_size, 1))\n",
    "        if diag == 1:\n",
    "            np.fill_diagonal(mask, 0)\n",
    "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        \n",
    "        # Scale E weight by E-I ratio\n",
    "        self.weight.data[:, :self.e_size] /= (self.e_size/self.i_size)\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def effective_weight(self):\n",
    "        return torch.abs(self.weight) * self.mask\n",
    "\n",
    "    def forward(self, input):\n",
    "        # weight is non-negative\n",
    "        return F.linear(input, self.effective_weight(), self.bias)\n",
    "\n",
    "\n",
    "class EIRNN(nn.Module):\n",
    "    \"\"\"E-I RNN.\n",
    "    \n",
    "    Reference:\n",
    "        Song, H.F., Yang, G.R. and Wang, X.J., 2016.\n",
    "        Training excitatory-inhibitory recurrent neural networks\n",
    "        for cognitive tasks: a simple and flexible framework.\n",
    "        PLoS computational biology, 12(2).\n",
    "\n",
    "    Args:\n",
    "        input_size: Number of input neurons\n",
    "        hidden_size: Number of hidden neurons\n",
    "\n",
    "    Inputs:\n",
    "        input: (seq_len, batch, input_size)\n",
    "        hidden: (batch, hidden_size)\n",
    "        e_prop: float between 0 and 1, proportion of excitatory neurons\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,input_size, hidden_size, dt=20,\n",
    "                 e_prop=0.8, sigma_rec=0.1, sigma_gain = 1,diag = 1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.e_size = int(hidden_size * e_prop)\n",
    "        self.i_size = hidden_size - self.e_size\n",
    "        self.num_layers = 1\n",
    "        self.tau = 100\n",
    "        if dt is None:\n",
    "            alpha = 1\n",
    "        else:\n",
    "            alpha = dt / self.tau\n",
    "        self.alpha = alpha\n",
    "        self.oneminusalpha = 1 - alpha\n",
    "        # Recurrent noise\n",
    "        self._sigma_rec = np.sqrt(dt) * sigma_rec\n",
    "        self.diag = diag;\n",
    "        self.input2h = PosWLinear(input_size, hidden_size)\n",
    "        # self.input2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2h = EIRecLinear(hidden_size, e_prop=e_prop,diag=diag)\n",
    "        self.gain = sigma_gain\n",
    " \n",
    "    def init_hidden(self, input):\n",
    "        batch_size = 1\n",
    "        return (torch.zeros(batch_size, self.hidden_size).to(input.device),\n",
    "                torch.zeros(batch_size, self.hidden_size).to(input.device))\n",
    "\n",
    "    def recurrence(self, input, hidden):\n",
    "        \"\"\"Recurrence helper.\"\"\"\n",
    "        \n",
    "        state, output = hidden # x(t) , r(t)\n",
    "        total_input = self.input2h(input) + self.h2h(output) # W_in * input + W_r * r\n",
    "        state = state * self.oneminusalpha + total_input * self.alpha # x(1-alpha) + # W_in * input + W_r * r\n",
    "        state += self._sigma_rec * torch.randn_like(state) # + noise\n",
    "        output = torch.sigmoid(self.gain*state)\n",
    "        return state, output\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        \"\"\"Propogate input through the network.\"\"\"\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(input)\n",
    "\n",
    "        output = []\n",
    "        steps = range(input.size(0))\n",
    "        for i in steps:\n",
    "            hidden = self.recurrence(input[i], hidden)\n",
    "            output.append(hidden[1])\n",
    "\n",
    "        output = torch.stack(output, dim=0)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \"\"\"Recurrent network model.\n",
    "\n",
    "    Args:\n",
    "        input_size: int, input size\n",
    "        hidden_size: int, hidden size\n",
    "        output_size: int, output size\n",
    "        rnn: str, type of RNN, lstm, rnn, ctrnn, or eirnn\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        # Excitatory-inhibitory RNN\n",
    "        self.rnn = EIRNN(input_size, hidden_size, **kwargs)\n",
    "        self.fc = PosWLinear(self.rnn.e_size, output_size)\n",
    "        # self.fc = nn.Linear(self.rnn.e_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        rnn_activity, _ = self.rnn(x)\n",
    "        rnn_e = rnn_activity[:, :, :self.rnn.e_size]\n",
    "        out = self.fc(rnn_e)\n",
    "        return out, rnn_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With everything set up we can now train the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network: 0\n",
      "torch.Size([50, 1, 2])\n",
      "torch.Size([50, 2])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network and print information\n",
    "hidden_size = 40; input_size = 2; output_size = 2\n",
    "e_size = .8; \n",
    "\n",
    "accuracy_cross_net = np.zeros([50])\n",
    "\n",
    "training_iterations = 1000\n",
    "for randseed in range(num_nets):\n",
    "    \n",
    "    net = Net(input_size=input_size, hidden_size=hidden_size, output_size=output_size,e_prop=e_size,diag=0)\n",
    "    \n",
    "    # Use Adam optimizer\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # trials\n",
    "    trl_type = np.tile([0,1],int(training_iterations/2))\n",
    "    np.random.shuffle(trl_type)\n",
    "    \n",
    "    running_loss = 0; running_acc = 0\n",
    "    print('Training network:',randseed)\n",
    "    \n",
    "    for i in range(training_iterations):\n",
    "        \n",
    "        trialtype = trl_type[i]\n",
    "        \n",
    "        # Generate input and target, convert to pytorch tensor\n",
    "        ob, gt, trial_type = ChangeEnvGen(trialtype)\n",
    "        \n",
    "        inputs = ob.T; labels = gt.T\n",
    "    \n",
    "        inputs = torch.from_numpy(inputs).type(torch.float)\n",
    "        labels = torch.from_numpy(labels.flatten()).type(torch.long)\n",
    "    \n",
    "        # boiler plate pytorch training:\n",
    "        optimizer.zero_grad() # zero the gradient buffers\n",
    "        output, _ = net(inputs)\n",
    "        print(output.shape)\n",
    "        output = torch.squeeze(output)\n",
    "        print(output.shape)\n",
    "        print(labels.shape)\n",
    "        # output = output.view(-1, output_size) # Reshape to (SeqLen x Batch, OutputSize)\n",
    "    \n",
    "        # cross entropy loss function\n",
    "        plt.plot(output.detach().numpy())\n",
    "        plt.plot(labels.detach().numpy())\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step() # Does the update\n",
    "        \n",
    "        # grab choice and compare to ground truth\n",
    "        choices = torch.argmax(output, dim=1) \n",
    "        acc = torch.mean(torch.eq(choices, labels).type(torch.float32)) # torch.eq computs elementwise equality\n",
    "    \n",
    "        # compute running accuracy and loss\n",
    "        running_loss = loss.item()\n",
    "        running_acc = acc.item()\n",
    "        \n",
    "        # Compute the running loss every 100 steps\n",
    "        if i % 100 == 99:\n",
    "            print('Step {}, Loss {:f}, Acc {:f}'.format(i+1, running_loss, running_acc))\n",
    "     \n",
    "    accuracy_cross_net[randseed] = running_acc\n",
    "    # save network weights    \n",
    "    varname = '/Users/chriswhyte/Documents/University/Projects/CurrentProjects/ChangeDetectionRNN/DaleNet/changedetection_weights_EI%d_%d.pth' % (randseed,hidden_size)\n",
    "    torch.save(net.state_dict(), varname)\n",
    "\n",
    "\n",
    "print('Training finished')\n",
    "\n",
    "mean_acc = np.mean(accuracy_cross_net)\n",
    "std_acc = np.std(accuracy_cross_net)\n",
    "print('Accuracy:  {:f} ± {:f}'. format(mean_acc, std_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert weights from tensors to numpy arrays and save as csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start weights conversion\n",
      "weight conversion finished\n"
     ]
    }
   ],
   "source": [
    "print('start weights conversion')\n",
    "for randseed in range(num_nets):\n",
    "\n",
    "    # Instantialise the network\n",
    "    hidden_size = 40; input_size = 2; output_size = 2\n",
    "    net = Net(input_size=input_size, hidden_size=hidden_size, output_size=output_size, dt=20)\n",
    "        \n",
    "    e_prop = 1;\n",
    "    e_size = int(e_prop * hidden_size)\n",
    "    i_size = hidden_size - e_size\n",
    "    mask = np.tile([1]*e_size+[-1]*i_size, (hidden_size, 1))\n",
    "    \n",
    "    # import pretrained networks\n",
    "    varname = '/Users/chriswhyte/Documents/University/Projects/CurrentProjects/ChangeDetectionRNN/DaleNet/ChangeDetectionWeightsEI/changedetection_weights_EI%d_%d.pth' % (randseed,hidden_size)\n",
    "    net.load_state_dict(torch.load(varname))\n",
    "    \n",
    "    # weights\n",
    "    W_in_EI = np.abs(np.squeeze(net.rnn.input2h.weight.detach().numpy()))\n",
    "    W_res_EI = net.rnn.h2h.weight.detach().numpy()\n",
    "    W_res_EI = np.abs(W_res_EI)*mask\n",
    "    W_out_EI = np.abs(net.fc.weight.detach().numpy())\n",
    "    \n",
    "    # save weights \n",
    "    np.savetxt(\"/Users/chriswhyte/Documents/University/Projects/CurrentProjects/ChangeDetectionRNN/DaleNet/EffectiveWeights/W_in_EI%d_%d\" % (randseed,hidden_size), W_in_EI, delimiter=\",\")\n",
    "    np.savetxt(\"/Users/chriswhyte/Documents/University/Projects/CurrentProjects/ChangeDetectionRNN/DaleNet/EffectiveWeights/W_res_EI%d_%d\" % (randseed,hidden_size), W_res_EI, delimiter=\",\")\n",
    "    np.savetxt(\"/Users/chriswhyte/Documents/University/Projects/CurrentProjects/ChangeDetectionRNN/DaleNet/EffectiveWeights/W_out_EI%d_%d\" % (randseed,hidden_size), W_out_EI, delimiter=\",\")\n",
    "\n",
    "print('weight conversion finished')\n",
    "\n",
    "# plt.imshow(W_res_EI, cmap='hot', interpolation='nearest')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
